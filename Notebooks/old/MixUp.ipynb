{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MixUp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogykranWg14s",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/yu4u/mixup-generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbCayVhcfDsm",
        "colab_type": "code",
        "outputId": "c1ade7fa-4329-43e1-d2e1-918a31a8f174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQd2EOqufFzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L-8fAi_fN23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = '/content/drive/My Drive/19 Fall/CSE 569/Project'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqr_mWJSfJ7t",
        "colab_type": "code",
        "outputId": "c0d30a83-71e3-4769-990c-8875099cc992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "X_train = pickle.load(open(ROOT + '/cifar/X_train', mode='rb'))\n",
        "print(X_train.shape)\n",
        "X_test = pickle.load(open(ROOT + '/cifar/X_test', mode='rb'))\n",
        "print(X_test.shape)\n",
        "X_val = pickle.load(open(ROOT + '/cifar/X_val', mode='rb'))\n",
        "print(X_val.shape)\n",
        "y_train = pickle.load(open(ROOT + '/cifar/y_train', mode='rb'))\n",
        "print(y_train.shape)\n",
        "y_test = pickle.load(open(ROOT + '/cifar/y_test', mode='rb'))\n",
        "print(y_test.shape)\n",
        "y_val = pickle.load(open(ROOT + '/cifar/y_val', mode='rb'))\n",
        "print(y_val.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(8000, 32, 32, 3)\n",
            "(32000, 10)\n",
            "(10000, 10)\n",
            "(8000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33t9d_8UfLl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import regularizers, optimizers\n",
        "from keras.datasets import cifar10\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq1fHnYafMOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions for plots\n",
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1, 2,figsize=(10, 5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'validation'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'validation'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAoEO3qtfRNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "1a6928c6-5776-42a3-c9d8-a154ff6c33a2"
      },
      "source": [
        "# Define the model\n",
        "num_filters = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(num_filters, (3,3), padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(num_filters, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(2*num_filters, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(2*num_filters, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_21 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 106,538\n",
            "Trainable params: 106,538\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtqoYQk3fUO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MixupGenerator():\n",
        "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, num_unlabled=28, shuffle=True, datagen=None):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.shuffle = shuffle\n",
        "        self.sample_num = len(X_train)\n",
        "        self.datagen = datagen\n",
        "        self.num_unlabled = num_unlabled\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            indexes = self.__get_exploration_order()\n",
        "            itr_num = int(len(indexes) // (self.batch_size))  # 32000 // 128 = 250\n",
        "\n",
        "            for i in range(itr_num):\n",
        "                batch_ids = indexes[i * self.batch_size:(i + 1) * self.batch_size] # [0:128], [128:256]\n",
        "                X, y = self.__data_generation(batch_ids)\n",
        "                \n",
        "                yield X, y\n",
        "\n",
        "    def __get_exploration_order(self):\n",
        "        indexes = np.arange(self.sample_num)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "        return indexes\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        _, h, w, c = self.X_train.shape\n",
        "        l = np.random.beta(self.alpha, self.alpha, self.num_unlabled // 2)\n",
        "        X_l = l.reshape(self.num_unlabled // 2, 1, 1, 1)\n",
        "        y_l = l.reshape(self.num_unlabled // 2, 1)\n",
        "\n",
        "        X_labeled = self.X_train[batch_ids[:(self.batch_size - self.num_unlabled)]]\n",
        "        X1 = self.X_train[batch_ids[(self.batch_size - self.num_unlabled):(self.batch_size - self.num_unlabled) + self.num_unlabled // 2]]\n",
        "        X2 = self.X_train[batch_ids[((self.batch_size - self.num_unlabled) + self.num_unlabled // 2):]]\n",
        "        X_1 = X1 * X_l + X2 * (1 - X_l)\n",
        "        X_2 = X2 * X_l + X1 * (1 - X_l)\n",
        "        X_unlabeled = np.concatenate((X_1, X_2), axis=0)\n",
        "        X = np.concatenate((X_labeled, X_unlabeled), axis=0)\n",
        "\n",
        "        if self.datagen:\n",
        "            for i in range(self.batch_size):\n",
        "                X[i] = self.datagen.random_transform(X[i])\n",
        "                X[i] = self.datagen.standardize(X[i])\n",
        "          \n",
        "\n",
        "        if isinstance(self.y_train, list):\n",
        "            y = []\n",
        "\n",
        "            for y_train_ in self.y_train:\n",
        "                y_labeled = y_train_[batch_ids[:(self.batch_size - self.num_unlabled)]]\n",
        "                y1 = y_train_[batch_ids[(self.batch_size - self.num_unlabled):(self.batch_size - self.num_unlabled) + self.num_unlabled // 2]]\n",
        "                y2 = y_train_[batch_ids[((self.batch_size - self.num_unlabled) + self.num_unlabled // 2):]]\n",
        "                y_1 = y1 * y_l + y2 * (1 - y_l)\n",
        "                y_2 = y2 * y_l + y1 * (1 - y_l)\n",
        "                y_unlabeled = np.concatenate((y_1, y_2), axis=0)\n",
        "                y.append(np.concatenate((y_labeled, y_unlabeled), axis=0))\n",
        "        else:\n",
        "            y_labeled = self.y_train[batch_ids[:(self.batch_size - self.num_unlabled)]]\n",
        "            y1 = self.y_train[batch_ids[(self.batch_size - self.num_unlabled):(self.batch_size - self.num_unlabled) + self.num_unlabled // 2]]\n",
        "            y2 = self.y_train[batch_ids[((self.batch_size - self.num_unlabled) + self.num_unlabled // 2):]]\n",
        "            y_1 = y1 * y_l + y2 * (1 - y_l)\n",
        "            y_2 = y2 * y_l + y1 * (1 - y_l)\n",
        "            y_unlabeled = np.concatenate((y_1, y_2), axis=0)\n",
        "            y = np.concatenate((y_labeled, y_unlabeled), axis=0)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmBkTTszfrCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXf2kt_OF9VD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik9MPIUPfuxR",
        "colab_type": "code",
        "outputId": "7d9895ea-77e2-4cff-8651-6b8fb0522ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training_generator = MixupGenerator(X_train, y_train, batch_size=128, alpha=0.2, datagen=datagen)()\n",
        "model.fit_generator(generator=training_generator,\n",
        "                    \n",
        "                    steps_per_epoch=250,  # 32000 // 128\n",
        "                    epochs=100,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 14s 57ms/step - loss: 1.8222 - acc: 0.3376 - val_loss: 1.4347 - val_acc: 0.4909\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 1.5045 - acc: 0.4686 - val_loss: 1.2789 - val_acc: 0.5338\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 1.3556 - acc: 0.5283 - val_loss: 1.1200 - val_acc: 0.5972\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 1.2435 - acc: 0.5701 - val_loss: 1.1359 - val_acc: 0.6024\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 1.1877 - acc: 0.5952 - val_loss: 0.9694 - val_acc: 0.6620\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 1.1309 - acc: 0.6178 - val_loss: 0.9822 - val_acc: 0.6589\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 1.0885 - acc: 0.6344 - val_loss: 0.9020 - val_acc: 0.6853\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 1.0459 - acc: 0.6498 - val_loss: 0.9037 - val_acc: 0.6919\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 1.0228 - acc: 0.6593 - val_loss: 0.8363 - val_acc: 0.7078\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.9920 - acc: 0.6705 - val_loss: 0.8113 - val_acc: 0.7255\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.9641 - acc: 0.6783 - val_loss: 0.8010 - val_acc: 0.7247\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 13s 54ms/step - loss: 0.9361 - acc: 0.6899 - val_loss: 0.7832 - val_acc: 0.7333\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.9256 - acc: 0.6960 - val_loss: 0.8044 - val_acc: 0.7268\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.9040 - acc: 0.7017 - val_loss: 0.7615 - val_acc: 0.7363\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8904 - acc: 0.7069 - val_loss: 0.7350 - val_acc: 0.7472\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8759 - acc: 0.7163 - val_loss: 0.7533 - val_acc: 0.7462\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8697 - acc: 0.7176 - val_loss: 0.7204 - val_acc: 0.7557\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8446 - acc: 0.7267 - val_loss: 0.7003 - val_acc: 0.7585\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8394 - acc: 0.7288 - val_loss: 0.7660 - val_acc: 0.7356\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8277 - acc: 0.7334 - val_loss: 0.7450 - val_acc: 0.7487\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8137 - acc: 0.7382 - val_loss: 0.6660 - val_acc: 0.7705\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.8032 - acc: 0.7422 - val_loss: 0.7026 - val_acc: 0.7626\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.8035 - acc: 0.7448 - val_loss: 0.6965 - val_acc: 0.7606\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7937 - acc: 0.7426 - val_loss: 0.8035 - val_acc: 0.7296\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.7862 - acc: 0.7485 - val_loss: 0.7077 - val_acc: 0.7623\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7764 - acc: 0.7501 - val_loss: 0.6908 - val_acc: 0.7635\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7710 - acc: 0.7527 - val_loss: 0.7072 - val_acc: 0.7581\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7686 - acc: 0.7565 - val_loss: 0.6457 - val_acc: 0.7774\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.7661 - acc: 0.7565 - val_loss: 0.6242 - val_acc: 0.7896\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7488 - acc: 0.7600 - val_loss: 0.6445 - val_acc: 0.7805\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.7478 - acc: 0.7612 - val_loss: 0.6269 - val_acc: 0.7888\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7461 - acc: 0.7621 - val_loss: 0.6383 - val_acc: 0.7828\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.7456 - acc: 0.7656 - val_loss: 0.6469 - val_acc: 0.7818\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.7279 - acc: 0.7709 - val_loss: 0.6108 - val_acc: 0.7950\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.7314 - acc: 0.7701 - val_loss: 0.6408 - val_acc: 0.7822\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.7216 - acc: 0.7715 - val_loss: 0.6181 - val_acc: 0.7924\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.7188 - acc: 0.7769 - val_loss: 0.6132 - val_acc: 0.7944\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.7083 - acc: 0.7768 - val_loss: 0.6127 - val_acc: 0.7954\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.7128 - acc: 0.7755 - val_loss: 0.5889 - val_acc: 0.8040\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.7120 - acc: 0.7739 - val_loss: 0.6178 - val_acc: 0.7920\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.7000 - acc: 0.7805 - val_loss: 0.6030 - val_acc: 0.7956\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6959 - acc: 0.7799 - val_loss: 0.6253 - val_acc: 0.7917\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.6940 - acc: 0.7812 - val_loss: 0.5913 - val_acc: 0.8032\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6951 - acc: 0.7834 - val_loss: 0.6167 - val_acc: 0.7921\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.6890 - acc: 0.7860 - val_loss: 0.5727 - val_acc: 0.8079\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.6886 - acc: 0.7860 - val_loss: 0.6087 - val_acc: 0.7973\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.6747 - acc: 0.7901 - val_loss: 0.6205 - val_acc: 0.7960\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6862 - acc: 0.7876 - val_loss: 0.5822 - val_acc: 0.8068\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6840 - acc: 0.7854 - val_loss: 0.5894 - val_acc: 0.8018\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6821 - acc: 0.7866 - val_loss: 0.5851 - val_acc: 0.8072\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6787 - acc: 0.7901 - val_loss: 0.6399 - val_acc: 0.7934\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6615 - acc: 0.7930 - val_loss: 0.6186 - val_acc: 0.7963\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 12s 50ms/step - loss: 0.6760 - acc: 0.7906 - val_loss: 0.5841 - val_acc: 0.8030\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6588 - acc: 0.7942 - val_loss: 0.6160 - val_acc: 0.7951\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6547 - acc: 0.7972 - val_loss: 0.5596 - val_acc: 0.8153\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 13s 53ms/step - loss: 0.6702 - acc: 0.7952 - val_loss: 0.5764 - val_acc: 0.8076\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6551 - acc: 0.7981 - val_loss: 0.5816 - val_acc: 0.8106\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6566 - acc: 0.8004 - val_loss: 0.6420 - val_acc: 0.7919\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6566 - acc: 0.7943 - val_loss: 0.5839 - val_acc: 0.8091\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6492 - acc: 0.8014 - val_loss: 0.6106 - val_acc: 0.8006\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6518 - acc: 0.8002 - val_loss: 0.5676 - val_acc: 0.8199\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6474 - acc: 0.7998 - val_loss: 0.5647 - val_acc: 0.8141\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6437 - acc: 0.8002 - val_loss: 0.5490 - val_acc: 0.8196\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6417 - acc: 0.8025 - val_loss: 0.5882 - val_acc: 0.8096\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6411 - acc: 0.8049 - val_loss: 0.6103 - val_acc: 0.8020\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6380 - acc: 0.8006 - val_loss: 0.5678 - val_acc: 0.8131\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6401 - acc: 0.8025 - val_loss: 0.5756 - val_acc: 0.8115\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 13s 52ms/step - loss: 0.6328 - acc: 0.8072 - val_loss: 0.5443 - val_acc: 0.8211\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 13s 51ms/step - loss: 0.6368 - acc: 0.8047 - val_loss: 0.6165 - val_acc: 0.7996\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6311 - acc: 0.8057 - val_loss: 0.5863 - val_acc: 0.8056\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 13s 50ms/step - loss: 0.6373 - acc: 0.8019 - val_loss: 0.5933 - val_acc: 0.8041\n",
            "Epoch 72/100\n",
            "224/250 [=========================>....] - ETA: 1s - loss: 0.6380 - acc: 0.8010"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIAssWCsgIJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}