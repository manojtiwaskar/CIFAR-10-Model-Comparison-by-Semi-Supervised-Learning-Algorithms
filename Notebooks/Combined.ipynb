{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Split_Data_Entropy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BMcwXUihCAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f98db76f-f751-488c-901b-9a4402c3c7c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTGm6rShQut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import regularizers, optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOlnvwPNhaTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT = '/content/drive/My Drive'\n",
        "\n",
        "def label_extract(x_array, y_array, label_split):\n",
        "  size = int(label_split * x_array.shape[0])\n",
        "  index = np.random.choice(x_array.shape[0], size , replace=False)\n",
        "  not_index = [x for x in range(x_array.shape[0]) if x not in index ]\n",
        "  x_labeled = x_array[index]\n",
        "  x_unlabeled = x_array[not_index] \n",
        "  y_labeled = y_array[index]\n",
        "  y_unlabeled = []\n",
        "  for i in range(len(not_index)):\n",
        "    y_unlabeled.append([-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
        "  y_unlabeled = np.array(y_unlabeled)\n",
        "  return x_labeled, y_labeled, x_unlabeled, y_unlabeled\n",
        "\n",
        "\n",
        "def data_split(train_label_split):\n",
        "  X_train0 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label0', mode='rb'))\n",
        "  X_train1 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label1', mode='rb'))\n",
        "  X_train2 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label2', mode='rb'))\n",
        "  X_train3 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label3', mode='rb'))\n",
        "  X_train4 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label4', mode='rb'))\n",
        "  X_train5 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label5', mode='rb'))\n",
        "  X_train6 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label6', mode='rb'))\n",
        "  X_train7 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label7', mode='rb'))\n",
        "  X_train8 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label8', mode='rb'))\n",
        "  X_train9 = pickle.load(open(ROOT + '/cifar/labeled/X_train_label9', mode='rb'))\n",
        "  \n",
        "  y_train0 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label0', mode='rb'))\n",
        "  y_train1 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label1', mode='rb'))\n",
        "  y_train2 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label2', mode='rb'))\n",
        "  y_train3 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label3', mode='rb'))\n",
        "  y_train4 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label4', mode='rb'))\n",
        "  y_train5 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label5', mode='rb'))\n",
        "  y_train6 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label6', mode='rb'))\n",
        "  y_train7 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label7', mode='rb'))\n",
        "  y_train8 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label8', mode='rb'))\n",
        "  y_train9 = pickle.load(open(ROOT + '/cifar/labeled/y_train_label9', mode='rb'))\n",
        "\n",
        "  X_train0_label, y_train0_label, X_train0_unlabel, y_train0_unlabel  = label_extract(X_train0, y_train0, train_label_split)\n",
        "  X_train1_label, y_train1_label, X_train1_unlabel, y_train1_unlabel  = label_extract(X_train1, y_train1, train_label_split)\n",
        "  X_train2_label, y_train2_label, X_train2_unlabel, y_train2_unlabel  = label_extract(X_train2, y_train2, train_label_split)\n",
        "  X_train3_label, y_train3_label, X_train3_unlabel, y_train3_unlabel  = label_extract(X_train3, y_train3, train_label_split)\n",
        "  X_train4_label, y_train4_label, X_train4_unlabel, y_train4_unlabel  = label_extract(X_train4, y_train4, train_label_split)\n",
        "  X_train5_label, y_train5_label, X_train5_unlabel, y_train5_unlabel  = label_extract(X_train5, y_train5, train_label_split)\n",
        "  X_train6_label, y_train6_label, X_train6_unlabel, y_train6_unlabel  = label_extract(X_train6, y_train6, train_label_split)\n",
        "  X_train7_label, y_train7_label, X_train7_unlabel, y_train7_unlabel  = label_extract(X_train7, y_train7, train_label_split)\n",
        "  X_train8_label, y_train8_label, X_train8_unlabel, y_train8_unlabel  = label_extract(X_train8, y_train8, train_label_split)\n",
        "  X_train9_label, y_train9_label, X_train9_unlabel, y_train9_unlabel  = label_extract(X_train9, y_train9, train_label_split)\n",
        "  \n",
        "  x_train = np.concatenate((X_train0_label, X_train1_label, X_train2_label, X_train3_label, X_train4_label, X_train5_label\n",
        "                                  , X_train6_label, X_train7_label, X_train8_label, X_train9_label, X_train0_unlabel, X_train1_unlabel, X_train2_unlabel, X_train3_unlabel, X_train4_unlabel, X_train5_unlabel\n",
        "                                  , X_train6_unlabel, X_train7_unlabel, X_train8_unlabel, X_train9_unlabel))\n",
        "  print(x_train.shape)\n",
        "  \n",
        "  y_train = np.concatenate((y_train0_label, y_train1_label, y_train2_label, y_train3_label, y_train4_label, y_train5_label\n",
        "                                  , y_train6_label, y_train7_label, y_train8_label, y_train9_label, y_train0_unlabel, y_train1_unlabel, y_train2_unlabel, y_train3_unlabel, y_train4_unlabel, y_train5_unlabel\n",
        "                                  , y_train6_unlabel, y_train7_unlabel, y_train8_unlabel, y_train9_unlabel))\n",
        "  print(y_train.shape)\n",
        "\n",
        "  return x_train, y_train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDaBUnCI7Ujd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "  \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def confusion_mat(true_val, pred_val):\n",
        "  confusion = confusion_matrix(true_val, pred_val)\n",
        "  return confusion\n",
        "\n",
        "def true_positive(confusion):\n",
        "  n = len(confusion)\n",
        "  tp = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    tp[i] = confusion[i][i]\n",
        "  return tp\n",
        "\n",
        "def false_positive(confusion, tp):\n",
        "  n = len(confusion)\n",
        "  s = np.sum(confusion, axis=0)\n",
        "  fp = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    fp[i] = s[i] - tp[i]\n",
        "  return fp\n",
        "\n",
        "def false_negative(confusion, tp):\n",
        "  n = len(confusion)\n",
        "  s = np.sum(confusion, axis=1)\n",
        "  fn = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    fn[i] = s[i] - tp[i]\n",
        "  return fn\n",
        "\n",
        "def true_negative(confusion, tp, fp, fn):\n",
        "  n = len(confusion)\n",
        "  s = np.sum(np.sum(confusion, axis=1))\n",
        "  tn = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    tn[i] = s - tp[i] - fp[i] - fn[i]\n",
        "  return tn\n",
        "\n",
        "def accuracy(confusion, tp, tn, fp, fn):\n",
        "  acc = (tp + tn)/(tp + tn + fp + fn)\n",
        "  acc_sum = np.sum(acc)\n",
        "  return acc_sum/len(confusion)\n",
        "\n",
        "def recall(confusion, tp, fn):\n",
        "  rec = (tp)/(tp + fn)\n",
        "  rec_sum = np.sum(rec)\n",
        "  return rec_sum/len(confusion)\n",
        "\n",
        "def precision(confusion, tp, fp):\n",
        "  prec = (tp)/(tp + fp)\n",
        "  prec_sum = np.sum(prec)\n",
        "  return prec_sum/len(confusion)\n",
        "\n",
        "def f1_score(confusion, precision, recall):\n",
        "  f1 = (2 * precision * recall)/(precision + recall)\n",
        "  f1_sum = np.sum(f1)\n",
        "  return f1_sum/len(confusion)\n",
        "\n",
        "def print_metrics(y_actu, y_pred):\n",
        "  confusion = confusion_mat(y_actu, y_pred)\n",
        "  tp = true_positive(confusion)\n",
        "  fp = false_positive(confusion, tp)\n",
        "  fn = false_negative(confusion, tp)\n",
        "  tn = true_negative(confusion, tp, fp, fn)\n",
        "  acc = accuracy(confusion, tp, tn, fp, fn)\n",
        "  rec = recall(confusion, tp, fn)\n",
        "  prec = precision(confusion, tp, fp)\n",
        "  f1 = f1_score(confusion, prec, rec)\n",
        "  print(confusion)\n",
        "  print('Accuracy: ', acc)\n",
        "  print('Recall: ', rec)\n",
        "  print('Precision: ', prec)\n",
        "  print('f1_score: ', f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHmguHcd5t-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(Y, A):\n",
        "  P = tensorflow.keras.backend.abs(Y)\n",
        "  P = tensorflow.keras.backend.min(P, axis=1)\n",
        "  # P = tensorflow.keras.backend.print_tensor(P, message=\"P is: \")\n",
        "\n",
        "  Y_l = tensorflow.boolean_mask(Y, tensorflow.equal(P, 0))\n",
        "  A_l = tensorflow.boolean_mask(A, tensorflow.equal(P, 0))\n",
        "  # A_l = tensorflow.keras.backend.print_tensor(A_l, message=\"A_l is: \")\n",
        "  A_l_log = tensorflow.keras.backend.log(A_l)\n",
        "  A_l = tensorflow.keras.backend.flatten(A_l)\n",
        "  A_l_log = tensorflow.keras.backend.flatten(A_l_log)\n",
        "  A_l_log = tensorflow.keras.backend.abs(A_l_log)\n",
        "  Y_l = tensorflow.keras.backend.flatten(Y_l)\n",
        "\n",
        "  # Y_l = tensorflow.keras.backend.print_tensor(Y_l, message=\"Y_l flattened is: \")\n",
        "  # A_l_log = tensorflow.keras.backend.print_tensor(A_l_log, message=\"A_l_log flattened is: \")\n",
        "\n",
        "  L_l = A_l_log * Y_l\n",
        "  L_l = tensorflow.boolean_mask(L_l, tensorflow.greater(L_l, 0))\n",
        "  L_l = tensorflow.keras.backend.mean(L_l)\n",
        "  # L_l = tensorflow.keras.backend.print_tensor(L_l, message=\"L_l is: \")\n",
        "\n",
        "  Y_u = tensorflow.boolean_mask(Y, tensorflow.greater(P, 0))\n",
        "  A_u = tensorflow.boolean_mask(A, tensorflow.greater(P, 0))\n",
        "  # Y_u = tensorflow.keras.backend.print_tensor(Y_u, message=\"Y_u is: \")\n",
        "  # A_u = tensorflow.keras.backend.print_tensor(A_u, message=\"A_u is: \")\n",
        "  A_u_log = tensorflow.keras.backend.log(A_u)\n",
        "  # A_u_log = tensorflow.keras.backend.print_tensor(A_u_log, message=\"A_u_log is: \")\n",
        "  A_u = tensorflow.keras.backend.flatten(A_u)\n",
        "  A_u_log = tensorflow.keras.backend.flatten(A_u_log)\n",
        "  A_u_log = tensorflow.keras.backend.abs(A_u_log)\n",
        "  L_u = A_u * A_u_log\n",
        "  L_u = tensorflow.keras.backend.mean(L_u)\n",
        "  # L_u = tensorflow.keras.backend.print_tensor(L_u, message=\"L_u is: \")\n",
        "\n",
        "  # L = 0 - L_l - L_u\n",
        "  L = L_l + L_u\n",
        "  # L = tensorflow.keras.backend.print_tensor(L, message=\"L is: \")\n",
        "  return L/1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDb3h3mU6J1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_save_model(x_train, y_train, x_test, y_test, train_label_split):\n",
        "  num_filters = 32\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(num_filters, (3,3), padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(num_filters, (3,3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Conv2D(2*num_filters, (3,3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(2*num_filters, (3,3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # Summarize the model\n",
        "  model.summary()\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "\n",
        "  # Train the model\n",
        "  start = time.time()\n",
        "  fit_history = model.fit(x_train, y_train,\n",
        "                        batch_size=128, epochs=1,\n",
        "                        validation_data = (x_test, y_test),\n",
        "                        verbose=1)\n",
        "  end = time.time()\n",
        "\n",
        "  # Training time\n",
        "  print(\"Model took %0.2f seconds to train\"%(end - start))\n",
        "  model_path = ROOT + '/cifar/model_entropy_label' + str(train_label_split)\n",
        "  model.save(model_path)\n",
        "  pred = model.predict(x_test, batch_size=128, verbose=1)\n",
        "  y_pred = pred.argmax(axis =-1)\n",
        "\n",
        "\n",
        "  y_test = pickle.load(open(ROOT + '/cifar/y_test', mode='rb'))\n",
        "  y_test = y_test.argmax(axis =-1)\n",
        "\n",
        "  print('Split value: ', train_label_split)\n",
        "  print_metrics(y_test, y_pred)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imae4_wo7YVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3de567c1-7716-481d-d0b6-a4790e3bd8dd"
      },
      "source": [
        "x_test = pickle.load(open(ROOT + '/cifar/X_test', mode='rb'))\n",
        "y_test = pickle.load(open(ROOT + '/cifar/y_test', mode='rb'))\n",
        "x_train1, y_train1 = data_split(0.5)\n",
        "x_train2, y_train2 = data_split(0.75)\n",
        "train_save_model(x_train1, y_train1, x_test, y_test, 0.5)\n",
        "train_save_model(x_train2, y_train2, x_test, y_test, 0.75)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 10)\n",
            "(50000, 32, 32, 3)\n",
            "(50000, 10)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 106,538\n",
            "Trainable params: 106,538\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 259s 5ms/sample - loss: 1.9355 - acc: 0.2286 - val_loss: nan - val_acc: 0.4824\n",
            "Model took 259.48 seconds to train\n",
            "10000/10000 [==============================] - 12s 1ms/sample\n",
            "Split value:  0.5\n",
            "[[428  28  36  38  38  39  23 113 154  89]\n",
            " [ 22 535   2  28   8  14  26  49  62 240]\n",
            " [ 41  14 148  93 151 190 183 165  18  35]\n",
            " [  8   9  17 335  40 298 126 116   8  32]\n",
            " [ 35   7  26  70 311 121 225 189   7  16]\n",
            " [  5   6  29 137  26 551  83 146   3   9]\n",
            " [  1   7  19  97  64  83 627  61   1  27]\n",
            " [  4  10  13  70  55 128  50 646   1  35]\n",
            " [ 91  56   9  24  10  25  13  38 600 112]\n",
            " [ 26  88   3  50   3  29  26 117  37 643]]\n",
            "Accuracy:  0.8964799999999998\n",
            "Recall:  0.4836842004219152\n",
            "Precision:  0.5050834223212955\n",
            "f1_score:  0.04941522470043234\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                40970     \n",
            "=================================================================\n",
            "Total params: 106,538\n",
            "Trainable params: 106,538\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 261s 5ms/sample - loss: 1.8052 - acc: 0.3307 - val_loss: nan - val_acc: 0.5438\n",
            "Model took 261.70 seconds to train\n",
            "10000/10000 [==============================] - 12s 1ms/sample\n",
            "Split value:  0.75\n",
            "[[544  35  65  19  44   6  29  41 165  38]\n",
            " [ 34 685  10  12  10   1  38  25  66 105]\n",
            " [ 81  10 389  39 171  64 156  81  28  19]\n",
            " [ 19  15  75 315  71 138 263  63  11  19]\n",
            " [ 36  14  92  39 470  45 179 109  15   8]\n",
            " [  5  12  70 159  66 370 143 158   3   9]\n",
            " [  6  15  57  27  76  11 745  30   8  12]\n",
            " [ 15   7  35  49  93  65  74 642   5  27]\n",
            " [ 96  60  26  11   9  11  18   7 708  32]\n",
            " [ 37 172  14  33  15   9  50  56  66 570]]\n",
            "Accuracy:  0.9087599999999998\n",
            "Recall:  0.5449161398558224\n",
            "Precision:  0.548613907262524\n",
            "f1_score:  0.05467587715664503\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
